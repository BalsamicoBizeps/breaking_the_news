{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f3896e1-400a-46eb-b921-908074f48c07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import glob\n",
    "from google.cloud import bigquery_storage\n",
    "import os\n",
    "from google.cloud import bigquery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "380600fe-fd0a-455d-9055-7a66721e841e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data with selected columns\n",
    "recent = pd.read_csv('/Users/reppmazc/Documents/IRONHACK/quests/final_project/news_data.csv',\n",
    "                     usecols=['url', 'publishedAt', 'content'])\n",
    "\n",
    "# Rename the 'publishedAt' column to 'datetime'\n",
    "recent = recent.rename(columns={'publishedAt': 'datetime'})\n",
    "\n",
    "# Define a function to format the datetime string to YYYYMMDDHHMMSS\n",
    "def format_datetime_string(dt_string):\n",
    "    # Remove timezone offset if it exists (e.g., \"+00:00\")\n",
    "    dt_string = dt_string.split('+')[0]\n",
    "    # Remove 'T' if present and join parts to get the desired format\n",
    "    dt_string = dt_string.replace('T', '').replace('-', '').replace(':', '').replace(' ', '')\n",
    "    return dt_string\n",
    "\n",
    "# Apply the function to the 'datetime' column\n",
    "recent['datetime'] = recent['datetime'].astype(str).apply(format_datetime_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0669f787-bf7a-46bf-9c27-c2b9ad663d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenating files -> all follow the same structure but have different rows\n",
    "# Define the path to your files\n",
    "file_path = '/Users/reppmazc/Documents/IRONHACK/quests/final_project/scraped_content_*.csv'\n",
    "\n",
    "# Use glob to get all file paths matching the pattern\n",
    "all_files = glob.glob(file_path)\n",
    "\n",
    "# Read each file and concatenate them into a single DataFrame\n",
    "df = pd.concat((pd.read_csv(f) for f in all_files), ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cfc368f2-a72e-40b9-b455-e607d1a3c1c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(67000, 2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "32c31883-1a8a-4a94-93c0-d03970ccbf71",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('/Users/reppmazc/Documents/IRONHACK/quests/final_project/all_articles_notcleaned.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e85113c1-6b68-49fe-ab6a-3b80883c0f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename specific columns\n",
    "df = df.rename(columns={'Content': 'content',\n",
    "                        'DocumentIdentifier': 'url'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "70bcd5db-9b5f-43f2-8617-cc1c324b7471",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(67000, 2)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bd2d8730-6327-45e7-bd29-4749f3983f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df, recent], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fcb33bda-d63c-44b8-babe-77a53786f6d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove duplicate rows going by the hyperlink in the 'DocumentIdentifier' column\n",
    "df = df.drop_duplicates(subset='url', keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c82cf5cb-67aa-435f-b330-07c2ba01a080",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(57000, 3)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d3006b43-1339-4765-989a-78260027e18b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove rows where the 'DocumentIdentifier' column contains dumb or non-german (speaking) outlets\n",
    "exclude_list = [\n",
    "    'www.urlaubspiraten.de', 'www.kino-zeit', 'www.macwelt', 'www.autobild', 'www.wrdw', \n",
    "    '.orf.at', 'diepresse.com', 'www.dw.com/es', 'www.dw.com/uk', 'www.wrdw.com/2024', \n",
    "    'www.dw.com/ru', 'www.dw.com/en', 'www.dw.com/pt-br', 'www.dw.com/tr', 'www.dw.com/sw', \n",
    "    'www.dw.com/fr', 'www.dw.com/bg', 'www.dw.com/sr', 'www.dw.com/bn', 'www.dw.com/mk', \n",
    "    'www.dw.com/pl', 'www.dw.com/ar', 'www.dw.com/ro', 'www.dw.com/hr', 'www.dw.com/id']\n",
    "\n",
    "# create a regex pattern by joining the list items with '|'\n",
    "pattern = '|'.join(exclude_list)\n",
    "\n",
    "# filter out rows where 'DocumentIdentifier' contains any of the substrings\n",
    "df = df[~df['url'].str.contains(pattern, na=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c522be00-4a5c-4cbf-ba90-0781c6eb4187",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(45117, 3)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3dd28b96-15d8-4a44-bc83-f30c05556f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace text in 'content' column by nan that contains freaky signs like these:\n",
    "freaky_pattern = r'Ã¼Ã¤Ã¶ÃŸâ¬Ã'\n",
    "\n",
    "# Replace rows in 'content' column with NaN where 'freaky' characters are found\n",
    "df['content'] = df['content'].apply(lambda x: np.nan if pd.notna(x) and bool(re.search(freaky_pattern, x)) else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "09b48deb-a3fe-472d-99ce-865cb88a3ab5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(45117, 3)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b649bb3e-08fd-4a6b-bb2c-56a0641e6f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking if text is german\n",
    "# common German words\n",
    "common_german_words = [\n",
    "    \"und\", \"der\", \"das\", \"ist\", \"zu\", \"mit\", \"von\", \"auf\", \"für\", \"den\", \"im\", \"ein\", \"nicht\",\n",
    "    \"eine\", \"als\", \"auch\", \"aber\", \"wie\", \"es\", \"am\", \"aus\", \"bei\", \"dass\", \"oder\", \"so\", \"wenn\",\n",
    "    \"werden\", \"wir\", \"hat\", \"sich\", \"dem\", \"des\", \"noch\", \"nur\", \"kann\", \"um\", \"ja\", \"mehr\"]\n",
    "\n",
    "# Higher threshold for minimum word matches\n",
    "threshold = 5\n",
    "\n",
    "# Check if a text is likely in German\n",
    "def is_german(text):\n",
    "    # Count occurrences of common German words\n",
    "    word_count = sum(word in text.lower() for word in common_german_words)\n",
    "    # Return NaN if it doesn't meet the threshold, otherwise return the text\n",
    "    return text if word_count >= threshold else np.nan\n",
    "\n",
    "# Apply function to the 'content' column\n",
    "df['content'] = df['content'].apply(lambda x: is_german(x) if isinstance(x, str) else np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c99600a4-65fd-4fec-b37f-1b3896b195cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(45117, 3)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "56d8fdc3-1913-4fb9-b392-bf5e8c050309",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    Im Streit über die sogenannte Gendersprache ha...\n",
      "1     Die Vorsitzenden der Grünen Jugend, Svenja Ap...\n",
      "2    Krieg in Nahost : Israels heikle Optionen\\n\\nE...\n",
      "3     Carsten Berger  Das Angebot kam plötzlich: 20...\n",
      "5     Bundesinnenministerin Faeser hat das vom Bund...\n",
      "Name: content, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# removing strings that are part of the article content of some cells\n",
    "# Define the non-article snippets and phrases to trim\n",
    "non_article_snippets = [\n",
    "    'Hauptnavigation: Nutzen Sie die Tabulatortaste, um durch die Menüpunkte zu navigieren. Öffnen Sie Untermenüs mit der Leertaste. Schließen Sie Untermenüs mit der Escape-Taste. Hauptnavigation: Nutzen Sie die Tabulatortaste, um durch die Menüpunkte zu navigieren. Öffnen Sie Untermenüs mit der Leertaste.',\n",
    "    'Lesen Sie mehr zum Thema In anspruchsvollen Berufsfeldern im Stellenmarkt der SZ. Sie möchten die digitalen Produkte der SZ mit uns weiterentwickeln? Bewerben Sie sich jetzt!Jobs bei der SZ Digitale Medien Gutscheine:',\n",
    "    'öffnet in neuem Tab oder Fenster',\n",
    "    'Danke, dass Sie ZEIT ONLINE nutzen.',\n",
    "    'Melden Sie sich jetzt mit Ihrem bestehenden Account an oder testen Sie unser digitales Abo mit Zugang zu allen Artikeln. Erscheinungsbild Die Zusammenfassung für diesen Artikel kann leider momentan nicht angezeigt werden.',\n",
    "    'ZEIT ONLINE hat diese Meldung redaktionell nicht bearbeitet. Sie wurde automatisch von der Deutschen Presse-Agentur (dpa) übernommen.',\n",
    "    'Kommentar | ',\n",
    "    'Berlin (dpa/bb). ',\n",
    "    '+++ ',\n",
    "    '++',\n",
    "    '© dpa-infocom, ',\n",
    "    'Drucken Teilen',\n",
    "    'Nicht verpassen: Alles rund ums Thema Job & Beruf finden Sie im Karriere-Newsletter unseres Partners Merkur.de.',\n",
    "    'Erstellt durch: ',\n",
    "    '0 Weniger als eine Minute',\n",
    "    '► ',\n",
    "    '© ',\n",
    "    '© dpa/',\n",
    "    '© REUTERS/',\n",
    "    '© Getty Images',\n",
    "    '© IMAGO/',\n",
    "    '© imago images/',\n",
    "    '© imago/',\n",
    "    'Lesen Sie auch',\n",
    "    '© Berliner Feuerwehr ',\n",
    "    '© privat ',\n",
    "    'Kopiere den aktuellen Link ',\n",
    "    'DIE ZEIT: ']\n",
    "\n",
    "phrases_to_trim_after = [\n",
    "    'Die WELT als ePaper: Die vollständige Ausgabe steht Ihnen bereits am Vorabend zur Verfügung – so sind Sie immer hochaktuell informiert. Weitere Informationen http://epaper.welt.de Der Kurz-Link dieses Artikels lautet:',\n",
    "    'Hier können Sie interessante Artikel speichern',\n",
    "    'Aktuelle Nachrichten und Hintergründe aus Politik, Wirtschaft und Sport aus Berlin, Deutschland und der Welt.']\n",
    "\n",
    "# Step 1: Remove specified snippets from within the content\n",
    "def remove_snippets(text):\n",
    "    if pd.notna(text):  # Ensure text is not NaN\n",
    "        for snippet in non_article_snippets:\n",
    "            text = text.replace(snippet, '')  # Remove each snippet if it appears\n",
    "    return text\n",
    "\n",
    "df['content'] = df['content'].apply(remove_snippets)\n",
    "\n",
    "# Step 2: Trim content after any of the specified phrases\n",
    "def trim_content(text):\n",
    "    if pd.notna(text):  # Ensure text is not NaN\n",
    "        for phrase in phrases_to_trim_after:\n",
    "            if phrase in text:\n",
    "                return text.split(phrase)[0]  # Keep only the part before the phrase\n",
    "    return text\n",
    "\n",
    "df['content'] = df['content'].apply(trim_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9e8fa626-60b4-40fd-939a-c943b9cf6e33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(45117, 3)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fd589865-95be-4ae7-ae7a-bf9093c14306",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    if isinstance(text, str):  # Ensure text is a string\n",
    "        text = re.sub(r\"http\\S+\", \"\", text)  # Remove URLs in general\n",
    "        text = re.sub(r\"©\\s?\\d+\", \"\", text)  # Remove copyright symbols\n",
    "        text = re.sub(r'\\n+', ' ', text)  # Replace newlines with spaces\n",
    "        text = re.sub(r'\\s+', ' ', text)  # Remove extra spaces\n",
    "        return text\n",
    "    return text  # Return NaN or non-string content as-is\n",
    "\n",
    "# Apply the cleaning function to each article\n",
    "df['content'] = df['content'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ab7d8243-b5c0-4c3f-aa0e-9a2ab63c8fd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(45117, 3)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5f4ca905-e0cc-4a06-addd-e7b103ffd540",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nan in cells with only whitespace\n",
    "df['content'] = df['content'].apply(lambda x: np.nan if isinstance(x, str) and x.strip() == '' else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "24505206-d987-4d27-a4c0-437494195926",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(45117, 3)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "25b26b2f-a4f8-455b-9d05-2db0ff9510dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_check = [col for col in df.columns if col != 'datetime']\n",
    "df = df.dropna(subset=columns_to_check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b8cd55ea-8683-4f5a-9170-79e2a1c61f82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(39677, 3)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b8729bf8-d611-4f6c-b5c2-721076923115",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new source column from hyperlink in 'DocumentIdentifier' column as string (source is between the first . and the second . in the hyperlink)\n",
    "df['source'] = df['url'].str.extract(r'\\.(\\w+)\\.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "30c994b9-9e31-49ef-a5c6-8822ba96e5b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(39677, 4)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1555507c-1fc7-427d-963b-fb536fb40beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# removed any non-approved sources\n",
    "allowed_sources = ['news', 'merkur', 'finanznachrichten', 'gruenderszene', 'hna', 'fnp', 'kreiszeitung', 'stern',\n",
    "                   'giessener-allgemeine', 'sueddeutsche', 'presseportal', 'gmx', 'op-online', 'tz', 'freenet',\n",
    "                   'giessener-anzeiger', 'come-on', 'n-tv', 'zeit', 'wallstreet-online', 'az-online', 'bild', 'rheinpfalz',\n",
    "                   'tagesspiegel', 'ad-hoc-news', 'badische-zeitung', 'faz', 'volksstimme', 'mannheim24', 'leinetal24',\n",
    "                   'schwarzwaelder-bote', 'shz', 'wn', 'fuldaerzeitung', 'ndr', 'ruhrnachrichten', 'stuttgarter-nachrichten',\n",
    "                   'donaukurier', 'nachrichten-heute', 'schwaebische', 'webnachrichten', 'hanauer', 'mittelbayerische', 'pnp',\n",
    "                   'rp-online', 'wz', 'nwzonline', 'lomazoma', 't-online', 'stuttgarter-zeitung', 'tag24', 'gn-online', 'onetz',\n",
    "                   'ruhr24', 'waz', 'welt', 'mz', 'saarbruecker-zeitung', 'dattelner-morgenpost', 'nn', 'aachener-zeitung',\n",
    "                   'bo', 'express', 'volksfreund', 'nw', 'noz', 'dorstenerzeitung', 'halternerzeitung', 'hellwegeranzeiger',\n",
    "                   'pz-news', 'augsburger-allgemeine', 'krzbb', 'web', 'morgenweb', 'wp', 'rnz', 'nordsee-zeitung',\n",
    "                   'mannheimer-morgen', 'swp', 'chip', 'tagesschau', 'nordkurier', 'abendblatt', 'nordbayern', 'zdf',\n",
    "                   'bnn', 'kn-online', 'abendzeitung-muenchen', 'spiegel', 'westfalen-blatt', 'insuedthueringen',\n",
    "                   'echo-online', 'mainpost', 'sport', 'frankenpost', 'lz', 'ka-news', 'weser-kurier', 'zvw', 'ln-online',\n",
    "                   'mdr', 'allgaeuer-zeitung', 'infranken', 'general-anzeiger-bonn', 'hochheimer-zeitung', 'otz',\n",
    "                   'wormser-zeitung', 'mt', 'derpatriot', 'wr', 'morgenpost', 'wochenblatt-reporter', 'gea',\n",
    "                   'goettinger-tageblatt', 'blick', 'neuepresse', 'on-online', 'waz-online', 'nrz', 'wired-de', 'fnweb',\n",
    "                   'kurier', 'dewezet', 'muensterschezeitung', 'mittelhessen', 'bergstraesser-anzeiger', 'paz-online',\n",
    "                   'schwetzinger-zeitung', 'braunschweiger-zeitung', 'ga', 'oz-online', 'nh24', 'lvz', 'op-marburg',\n",
    "                   'cannstatter-zeitung', 'derstandard', 'wirtschafts-woche', 'solinger-tageblatt', 'derwesten', 'swr',\n",
    "                   'mopo', 'antenneniederrhein', 'berliner-zeitung', 'freiepresse', 'deutschlandfunk', 'maz-online',\n",
    "                   'handelsblatt', 'stimme', 'radioerft', 'thueringer-allgemeine', 'tlz', 'esslinger-zeitung', 'pressebox',\n",
    "                   'radiokoeln', 'ikz-online', 'radioenneperuhr', 'radio-bamberg', 'radioeins', 'gala', 'radio-plassenburg',\n",
    "                   'lampertheimer-zeitung', 'radio912', 'rga', 'rnd', 'sn-online', 'dnn', 'radioeuskirchen', 'bbv-net',\n",
    "                   'sat1regional', 'taz', 'idowa', 'lippewelle', 'radiokiepenkerl', 'hellwegradio', 'radiooberhausen',\n",
    "                   'soester-anzeiger', 'bunte', 'moin', 'rtl', 'antennemuenster', 'businessinsider', 'news38', 'main-netz',\n",
    "                   'harzkurier', 'radiomk', 'hessenschau', 'presse-board', 'radio901', 'iz', 'moz', 'br', 'lkz', 'sport1',\n",
    "                   'thueringen24', 'epochtimes', 'haz', 'radiokw', 'lr-online', 'prosieben', 'np-coburg', 'obermain',\n",
    "                   'presse-augsburg', 'lokalkompass', 'osthessen-news', 'unternehmen-heute', 'news894', 'ejz', 'city-news',\n",
    "                   'ffh', 'idw-online', 'radiobonn', 'radiomuelheim', 'radiowmw', 'goslarsche', 'radiorst', 'wdr',\n",
    "                   'hersfelder-zeitung', 'schlaunews', 'brigitte', 'heise', 'fr', 'radiohagen', 'radiovest', 'watson',\n",
    "                   'berliner-kurier', 'regio-journal', 'sportschau', 'berliner-abendblatt', 'tageblatt',\n",
    "                   'oldenburger-onlinezeitung', 'wiesbadener-kurier', 'bietigheimerzeitung', 'rosenheim24', 'deraktionaer',\n",
    "                   'familie', 'rbb24', 'sat1', 'all-in', 'deutschlandfunkkultur', 'presseschleuder', 'saechsische', 'vip',\n",
    "                   'azonline', 'jungewelt', 'spox', 'nd-aktuell', 'schwaebische-post', 'allgemeine-zeitung', 'rollingpin',\n",
    "                   'aerztezeitung', 'juedische-allgemeine', 'innsalzach24', 'buerstaedter-zeitung', 'mein-mmo', 'notebookcheck',\n",
    "                   'winfuture', 'borkenerzeitung', 'dubisthalle', 'moritz', 'openpr', 'presse-nachrichten', 'chiemgau24', 'dw',\n",
    "                   'echo24', 'hasepost', 'inar', 'kreisbote', 'artikel-presse', 'hildesheimer-allgemeine', 'presseradar',\n",
    "                   'radio38', 'shk-journal', 'turi2', 'unsertirol24', 'fcbinside', 'siegener-zeitung', 'topagrar',\n",
    "                   'xn--brgersagt-q9a', 'focus', 'pressnetwork', 'stadt-bremerhaven', 't3n', 'deutsche-wirtschafts-nachrichten',\n",
    "                   'die-neue-welle', 'wetterauer-zeitung', 'contentmanager', 'journalistenwatch', 'manager-magazin', 'ovb-online',\n",
    "                   'rollingstone', 'sol', 'kreis-anzeiger', 'wiesentbote', 'bgland24', 'prisma', 'capital', 'freitag', 'inside-digital',\n",
    "                   'kabeleins', 'nord24', 'ramasuri', 'sauerlandkurier', 'lokalo', 'telepolis', 'tichyseinblick', 'usinger-anzeiger',\n",
    "                   'euractiv', 'geo', 'karlsruhe-insider', 'lebensmittelzeitung', 'ostsee-zeitung', 'pressetext', 'weltjournal',\n",
    "                   'futurezone', 'fvw', 'gamereactor', 'gmuender-tagespost', 'hl-live', 'news8', 'unitednetworker', '24rhein',\n",
    "                   'jungefreiheit', 'mmnews', 'pfalz-express', 'pressecontrol', 'wa', 'ak-kurier', 'haufe', 'katholisch', 'ludwigsburg24',\n",
    "                   'ukrinform', 'viply', 'ahgz', 'appgefahren', 'berlin', 'daserste', 'fuldainfo', 'hallo-muenchen', 'hz', 'lifepr',\n",
    "                   'neckaralblive', 'ok-magazin', 'play3', 'macerkopf', 'wlz-online', 'ingame', 'l-iz', 'lauterbacher-anzeiger', 'mrn-news',\n",
    "                   'nrwz', 'harpersbazaar', 'hightechbox', 'mainfranken24', 'radioleverkusen', 'report-k', 'wochenkurier', 'irw-press',\n",
    "                   'monopol-magazin', 'niederlausitz-aktuell', 'ntz', 'sonntagsblatt', 'spektrum', 'cicero', 'oekotest', 'vogel', 'buzzfeed',\n",
    "                   'die-tagespost', 'donau3fm', 'e110', 'medical-tribune', 'rdl', 'rundblick-unna', 'ww-kurier', 'aero', 'monstersandcritics',\n",
    "                   'nr-kurier', 'rotenburger-rundschau', 'swr3', 'wz-net', 'asscompact', 'brn-ag', 'deichstube', 'esut', 'kraichgau',\n",
    "                   'ludwigshafen24', 'mittelrhein-tageblatt', 'oberhessische-zeitung', 'scinexx', 'szbz', 'teltarif', 'travelbook',\n",
    "                   'wirtschaft-in-sachsen', 'ardmediathek', 'areadvd', 'baunetz', 'behoerden-spiegel', 'bergkamen-infoblog', 'harburg-aktuell',\n",
    "                   'lto', 'raptastisch', 'reitschuster', 'remszeitung', 'report24', 'riffreporter', 'springermedizin', 'vdi-nachrichten',\n",
    "                   'bmvg', 'cio', 'dawo-dresden', 'dbwv', 'dzonline', 'elektro', 'gaeubote', 'jesus', 'landeswelle', 'mena-watch',\n",
    "                   'nachrichten-muenchen', 'netzpolitik', 'nordstadtblogger', 'nuernberger-blatt', 'politikstube', 'queer', 'theeuropean',\n",
    "                   'volcanodiscovery', 'werra-rundschau', 'wochenanzeiger', 'absatzwirtschaft', 'bernau-live', 'buffed', 'business-on',\n",
    "                   'charivari', 'epd', 'gandersheimer-kreisblatt', 'garbsen-city-news', 'intelligent-investors', 'invidis', 'kma-online',\n",
    "                   'marler-zeitung', 'migazin', 'norderneyer-badezeitung', 'paz', 'radio-rur', 'regenbogen', 'rtf1', 'stylebook',\n",
    "                   'taunus-nachrichten', 'utopia', 'vogue', 'ad-magazin', 'bayreuth', 'blog-der-republik', 'bz-berlin', 'einbecker-morgenpost',\n",
    "                   'esanum', 'hcm-magazin', 'heidelberg24', 'helmholtz', 'honnef-heute', 'ingolstadt-today', 'israelnetz', 'iww',\n",
    "                   'kevelaerer-blatt', 'klimareporter', 'muehlacker-tagblatt', 'muensterlandzeitung', 'onvista', 'opposition24',\n",
    "                   'politplatschquatsch', 'rbb-online', 'regensburger-nachrichten', 'rheiderland', 'rohmert-medien', 'solarserver',\n",
    "                   'st-georg', 'tip-berlin', 'unternehmeredition', 'alfelder-zeitung', 'b2b-wirtschaft', 'bba-online', 'berlin-live',\n",
    "                   'business-echo', 'darmstadtnews', 'desired', 'dieunbestechlichen', 'docma', 'edison', 'electrive', 'elle',\n",
    "                   'ems-vechte-surfer', 'emtb-news', 'glamour', 'heide-kurier', 'hr-fernsehen', 'kiel-magazin', 'lokal-anzeiger-erkrath',\n",
    "                   'main-spitze', 'meine-news', 'merkurist', 'moeckern24', 'mpg', 'nationalgeographic', 'osna-live', 'paymentandbanking',\n",
    "                   'pnn', 'prad', 'pta-in-love', 'pv-magazine', 'rblive', 'redspa', 'reporter-ohne-grenzen', 'rosalux', 'sid', 'sifa-sibe',\n",
    "                   'svz', 'traunsteiner-tagblatt', 'uebermedien']\n",
    "\n",
    "df = df[df['source'].isin(allowed_sources)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b05eebac-02d8-4215-94a8-d2eab3a0422a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(37782, 4)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "763eb35a-2265-4a90-abd9-ab2bf45500f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('/Users/reppmazc/Documents/IRONHACK/quests/final_project/cleaned_articles_wo_date.csv')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ee023180-3146-4dcb-afb0-7edb16bf908e",
   "metadata": {},
   "source": [
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"xxx\"\n",
    "client = bigquery.Client()\n",
    "\n",
    "# Function to get datetime for each URL, preserving existing values if not found\n",
    "def get_datetime(url, current_datetime):\n",
    "    query = \"\"\"\n",
    "    SELECT DATE\n",
    "    FROM `fourth-epigram-440716-f1.news_analysis.gdelt`\n",
    "    WHERE DocumentIdentifier = @url\n",
    "    LIMIT 1\n",
    "    \"\"\"\n",
    "    # Use parameterized query to prevent SQL injection\n",
    "    job_config = bigquery.QueryJobConfig(\n",
    "        query_parameters=[\n",
    "            bigquery.ScalarQueryParameter(\"url\", \"STRING\", url)])\n",
    "    # Run the query and fetch results\n",
    "    result = client.query(query, job_config=job_config).to_dataframe()\n",
    "    \n",
    "    # Return the date if found, otherwise keep the existing datetime\n",
    "    return result['DATE'].iloc[0] if not result.empty else current_datetime\n",
    "\n",
    "# Test the function on a small chunk (first 10 rows)\n",
    "test_df = df.sample(n=10, random_state=42).copy()\n",
    "\n",
    "# Update the 'datetime' column in the test DataFrame\n",
    "test_df['datetime'] = test_df.apply(lambda row: get_datetime(row['url'], row['datetime']), axis=1)\n",
    "\n",
    "# Check the updated test DataFrame\n",
    "print(test_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "881df12c-b260-43ca-acbc-7c84bd29df04",
   "metadata": {},
   "outputs": [
    {
     "ename": "Forbidden",
     "evalue": "403 Quota exceeded: Your project exceeded quota for free query bytes scanned. For more information, see https://cloud.google.com/bigquery/docs/troubleshoot-quotas; reason: quotaExceeded, location: unbilled.analysis, message: Quota exceeded: Your project exceeded quota for free query bytes scanned. For more information, see https://cloud.google.com/bigquery/docs/troubleshoot-quotas\n\nLocation: US\nJob ID: 4937227c-6aa4-445c-ad44-f2ea9fd5731b\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mForbidden\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 24\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDATE\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m result\u001b[38;5;241m.\u001b[39mempty \u001b[38;5;28;01melse\u001b[39;00m current_datetime\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Update the 'datetime' column in place, preserving existing values where URLs are not found\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdatetime\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mget_datetime\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43murl\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdatetime\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/pandas/core/frame.py:10034\u001b[0m, in \u001b[0;36mDataFrame.apply\u001b[0;34m(self, func, axis, raw, result_type, args, by_row, **kwargs)\u001b[0m\n\u001b[1;32m  10022\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapply\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m frame_apply\n\u001b[1;32m  10024\u001b[0m op \u001b[38;5;241m=\u001b[39m frame_apply(\n\u001b[1;32m  10025\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m  10026\u001b[0m     func\u001b[38;5;241m=\u001b[39mfunc,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m  10032\u001b[0m     kwargs\u001b[38;5;241m=\u001b[39mkwargs,\n\u001b[1;32m  10033\u001b[0m )\n\u001b[0;32m> 10034\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapply\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/pandas/core/apply.py:837\u001b[0m, in \u001b[0;36mFrameApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    834\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw:\n\u001b[1;32m    835\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_raw()\n\u001b[0;32m--> 837\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/pandas/core/apply.py:965\u001b[0m, in \u001b[0;36mFrameApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    964\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_standard\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 965\u001b[0m     results, res_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_series_generator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    967\u001b[0m     \u001b[38;5;66;03m# wrap results\u001b[39;00m\n\u001b[1;32m    968\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwrap_results(results, res_index)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/pandas/core/apply.py:981\u001b[0m, in \u001b[0;36mFrameApply.apply_series_generator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    978\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m option_context(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmode.chained_assignment\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    979\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(series_gen):\n\u001b[1;32m    980\u001b[0m         \u001b[38;5;66;03m# ignore SettingWithCopy here in case the user mutates\u001b[39;00m\n\u001b[0;32m--> 981\u001b[0m         results[i] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    982\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(results[i], ABCSeries):\n\u001b[1;32m    983\u001b[0m             \u001b[38;5;66;03m# If we have a view on v, we need to make a copy because\u001b[39;00m\n\u001b[1;32m    984\u001b[0m             \u001b[38;5;66;03m#  series_generator will swap out the underlying data\u001b[39;00m\n\u001b[1;32m    985\u001b[0m             results[i] \u001b[38;5;241m=\u001b[39m results[i]\u001b[38;5;241m.\u001b[39mcopy(deep\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[31], line 24\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(row)\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDATE\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m result\u001b[38;5;241m.\u001b[39mempty \u001b[38;5;28;01melse\u001b[39;00m current_datetime\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Update the 'datetime' column in place, preserving existing values where URLs are not found\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdatetime\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m row: \u001b[43mget_datetime\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43murl\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdatetime\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "Cell \u001b[0;32mIn[31], line 18\u001b[0m, in \u001b[0;36mget_datetime\u001b[0;34m(url, current_datetime)\u001b[0m\n\u001b[1;32m     14\u001b[0m job_config \u001b[38;5;241m=\u001b[39m bigquery\u001b[38;5;241m.\u001b[39mQueryJobConfig(\n\u001b[1;32m     15\u001b[0m     query_parameters\u001b[38;5;241m=\u001b[39m[\n\u001b[1;32m     16\u001b[0m         bigquery\u001b[38;5;241m.\u001b[39mScalarQueryParameter(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124murl\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSTRING\u001b[39m\u001b[38;5;124m\"\u001b[39m, url)])\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Run the query and fetch results\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjob_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjob_config\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_dataframe\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Return the date if found, otherwise keep the existing datetime\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDATE\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m result\u001b[38;5;241m.\u001b[39mempty \u001b[38;5;28;01melse\u001b[39;00m current_datetime\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/google/cloud/bigquery/job/query.py:2057\u001b[0m, in \u001b[0;36mQueryJob.to_dataframe\u001b[0;34m(self, bqstorage_client, dtypes, progress_bar_type, create_bqstorage_client, max_results, geography_as_object, bool_dtype, int_dtype, float_dtype, string_dtype, date_dtype, datetime_dtype, time_dtype, timestamp_dtype, range_date_dtype, range_datetime_dtype, range_timestamp_dtype)\u001b[0m\n\u001b[1;32m   1827\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mto_dataframe\u001b[39m(\n\u001b[1;32m   1828\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1829\u001b[0m     bqstorage_client: Optional[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbigquery_storage.BigQueryReadClient\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1849\u001b[0m     ] \u001b[38;5;241m=\u001b[39m DefaultPandasDTypes\u001b[38;5;241m.\u001b[39mRANGE_TIMESTAMP_DTYPE,\n\u001b[1;32m   1850\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpandas.DataFrame\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1851\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return a pandas DataFrame from a QueryJob\u001b[39;00m\n\u001b[1;32m   1852\u001b[0m \n\u001b[1;32m   1853\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2055\u001b[0m \u001b[38;5;124;03m            :mod:`shapely` library cannot be imported.\u001b[39;00m\n\u001b[1;32m   2056\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2057\u001b[0m     query_result \u001b[38;5;241m=\u001b[39m \u001b[43mwait_for_query\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprogress_bar_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_results\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_results\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2058\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m query_result\u001b[38;5;241m.\u001b[39mto_dataframe(\n\u001b[1;32m   2059\u001b[0m         bqstorage_client\u001b[38;5;241m=\u001b[39mbqstorage_client,\n\u001b[1;32m   2060\u001b[0m         dtypes\u001b[38;5;241m=\u001b[39mdtypes,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2074\u001b[0m         range_timestamp_dtype\u001b[38;5;241m=\u001b[39mrange_timestamp_dtype,\n\u001b[1;32m   2075\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/google/cloud/bigquery/_tqdm_helpers.py:107\u001b[0m, in \u001b[0;36mwait_for_query\u001b[0;34m(query_job, progress_bar_type, max_results)\u001b[0m\n\u001b[1;32m    103\u001b[0m progress_bar \u001b[38;5;241m=\u001b[39m get_progress_bar(\n\u001b[1;32m    104\u001b[0m     progress_bar_type, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQuery is running\u001b[39m\u001b[38;5;124m\"\u001b[39m, default_total, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquery\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    105\u001b[0m )\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m progress_bar \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 107\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mquery_job\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_results\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_results\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    109\u001b[0m i \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/google/cloud/bigquery/job/query.py:1681\u001b[0m, in \u001b[0;36mQueryJob.result\u001b[0;34m(self, page_size, max_results, retry, timeout, start_index, job_retry)\u001b[0m\n\u001b[1;32m   1676\u001b[0m     remaining_timeout \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1678\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m remaining_timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1679\u001b[0m     \u001b[38;5;66;03m# Since is_job_done() calls jobs.getQueryResults, which is a\u001b[39;00m\n\u001b[1;32m   1680\u001b[0m     \u001b[38;5;66;03m# long-running API, don't delay the next request at all.\u001b[39;00m\n\u001b[0;32m-> 1681\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mis_job_done\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   1682\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m   1683\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1684\u001b[0m     \u001b[38;5;66;03m# Use a monotonic clock since we don't actually care about\u001b[39;00m\n\u001b[1;32m   1685\u001b[0m     \u001b[38;5;66;03m# daylight savings or similar, just the elapsed time.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/google/api_core/retry/retry_unary.py:293\u001b[0m, in \u001b[0;36mRetry.__call__.<locals>.retry_wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    289\u001b[0m target \u001b[38;5;241m=\u001b[39m functools\u001b[38;5;241m.\u001b[39mpartial(func, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    290\u001b[0m sleep_generator \u001b[38;5;241m=\u001b[39m exponential_sleep_generator(\n\u001b[1;32m    291\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initial, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maximum, multiplier\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_multiplier\n\u001b[1;32m    292\u001b[0m )\n\u001b[0;32m--> 293\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mretry_target\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_predicate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43msleep_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    298\u001b[0m \u001b[43m    \u001b[49m\u001b[43mon_error\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mon_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    299\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/google/api_core/retry/retry_unary.py:153\u001b[0m, in \u001b[0;36mretry_target\u001b[0;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;66;03m# This function explicitly must deal with broad exceptions.\u001b[39;00m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    152\u001b[0m     \u001b[38;5;66;03m# defer to shared logic for handling errors\u001b[39;00m\n\u001b[0;32m--> 153\u001b[0m     \u001b[43m_retry_error_helper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdeadline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[43m        \u001b[49m\u001b[43msleep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[43m        \u001b[49m\u001b[43merror_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpredicate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[43m        \u001b[49m\u001b[43mon_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexception_factory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;66;03m# if exception not raised, sleep before next attempt\u001b[39;00m\n\u001b[1;32m    164\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(sleep)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/google/api_core/retry/retry_base.py:212\u001b[0m, in \u001b[0;36m_retry_error_helper\u001b[0;34m(exc, deadline, next_sleep, error_list, predicate_fn, on_error_fn, exc_factory_fn, original_timeout)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m predicate_fn(exc):\n\u001b[1;32m    207\u001b[0m     final_exc, source_exc \u001b[38;5;241m=\u001b[39m exc_factory_fn(\n\u001b[1;32m    208\u001b[0m         error_list,\n\u001b[1;32m    209\u001b[0m         RetryFailureReason\u001b[38;5;241m.\u001b[39mNON_RETRYABLE_ERROR,\n\u001b[1;32m    210\u001b[0m         original_timeout,\n\u001b[1;32m    211\u001b[0m     )\n\u001b[0;32m--> 212\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m final_exc \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msource_exc\u001b[39;00m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m on_error_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    214\u001b[0m     on_error_fn(exc)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/google/api_core/retry/retry_unary.py:144\u001b[0m, in \u001b[0;36mretry_target\u001b[0;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sleep \u001b[38;5;129;01min\u001b[39;00m sleep_generator:\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 144\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mtarget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    145\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39misawaitable(result):\n\u001b[1;32m    146\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(_ASYNC_RETRY_WARNING)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/google/cloud/bigquery/job/query.py:1630\u001b[0m, in \u001b[0;36mQueryJob.result.<locals>.is_job_done\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1607\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m job_failed_exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1608\u001b[0m     \u001b[38;5;66;03m# Only try to restart the query job if the job failed for\u001b[39;00m\n\u001b[1;32m   1609\u001b[0m     \u001b[38;5;66;03m# a retriable reason. For example, don't restart the query\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1627\u001b[0m     \u001b[38;5;66;03m# into an exception that can be processed by the\u001b[39;00m\n\u001b[1;32m   1628\u001b[0m     \u001b[38;5;66;03m# `job_retry` predicate.\u001b[39;00m\n\u001b[1;32m   1629\u001b[0m     restart_query_job \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m-> 1630\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m job_failed_exception\n\u001b[1;32m   1631\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1632\u001b[0m     \u001b[38;5;66;03m# Make sure that the _query_results are cached so we\u001b[39;00m\n\u001b[1;32m   1633\u001b[0m     \u001b[38;5;66;03m# can return a complete RowIterator.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1639\u001b[0m     \u001b[38;5;66;03m# making any extra API calls if the previous loop\u001b[39;00m\n\u001b[1;32m   1640\u001b[0m     \u001b[38;5;66;03m# iteration fetched the finished job.\u001b[39;00m\n\u001b[1;32m   1641\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reload_query_results(\n\u001b[1;32m   1642\u001b[0m         retry\u001b[38;5;241m=\u001b[39mretry, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mreload_query_results_kwargs\n\u001b[1;32m   1643\u001b[0m     )\n",
      "\u001b[0;31mForbidden\u001b[0m: 403 Quota exceeded: Your project exceeded quota for free query bytes scanned. For more information, see https://cloud.google.com/bigquery/docs/troubleshoot-quotas; reason: quotaExceeded, location: unbilled.analysis, message: Quota exceeded: Your project exceeded quota for free query bytes scanned. For more information, see https://cloud.google.com/bigquery/docs/troubleshoot-quotas\n\nLocation: US\nJob ID: 4937227c-6aa4-445c-ad44-f2ea9fd5731b\n"
     ]
    }
   ],
   "source": [
    "# get datetime data from gdelt rows matched based on content of 'DocumentIdentifier' save in new datetime column \n",
    "# Set up Google Cloud credentials and BigQuery client\n",
    "df = pd.read_csv('') # @Janos add the path to the csv file i airdropped you yesterday\n",
    "\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"xx\"\n",
    "client = bigquery.Client()\n",
    "\n",
    "def get_datetime(url, current_datetime):\n",
    "    query = \"\"\"\n",
    "    SELECT DATE\n",
    "    FROM `fourth-epigram-440716-f1.news_analysis.gdelt`\n",
    "    WHERE DocumentIdentifier = @url\n",
    "    LIMIT 1\n",
    "    \"\"\"\n",
    "    # Use parameterized query to prevent SQL injection\n",
    "    job_config = bigquery.QueryJobConfig(\n",
    "        query_parameters=[\n",
    "            bigquery.ScalarQueryParameter(\"url\", \"STRING\", url)])\n",
    "    # Run the query and fetch results\n",
    "    result = client.query(query, job_config=job_config).to_dataframe()\n",
    "    \n",
    "    # Return the date if found, otherwise keep the existing datetime\n",
    "    return result['DATE'].iloc[0] if not result.empty else current_datetime\n",
    "\n",
    "# Update the 'datetime' column in place, preserving existing values where URLs are not found\n",
    "df['datetime'] = df.apply(lambda row: get_datetime(row['url'], row['datetime']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01ef068-8a1d-4b71-9421-1ce2bda667cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('/Users/reppmazc/Documents/IRONHACK/quests/final_project/cleaned_articles_w_date.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de9973ba-e1d8-48de-a376-37b3ef3db940",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['source'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b31508-5a0f-41ab-8c52-b441eda71af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (news_project)",
   "language": "python",
   "name": "news_project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
