{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c71330fb-063d-4782-92dd-02bb91faf265",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.utils import simple_preprocess\n",
    "from transformers import pipeline, AutoTokenizer\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a918b8da-152e-4c34-ab0e-d6f14c9ff736",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading article data\n",
    "df = pd.read_csv(\"/Users/reppmazc/Documents/IRONHACK/quests/final_project/cleaned_articles_wo_date.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec65302c-81c9-4917-b8ff-bc006455787b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test three different models on 100 randomly selected articles\n",
    "# Ensure stopwords are downloaded\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('german'))\n",
    "\n",
    "# Step 1: Define text preprocessing function\n",
    "def preprocess_text(text):\n",
    "    # Remove numbers and punctuation, lowercase, and remove stopwords\n",
    "    text = re.sub(r'\\d+', '', text)  # Remove numbers\n",
    "    text = re.sub(r'\\W', ' ', text)  # Remove punctuation\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    tokens = simple_preprocess(text)  # Tokenize\n",
    "    tokens = [word for word in tokens if word not in stop_words]  # Remove stopwords\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Apply preprocessing to the 'content' column\n",
    "df['processed_content'] = df['content'].apply(preprocess_text)\n",
    "\n",
    "# Step 2: Select a random sample of 100 articles\n",
    "sample_df = df.sample(n=100, random_state=1)\n",
    "\n",
    "# Step 3: Load tokenizers and define truncation for each model\n",
    "model_names = {\"XLM-RoBERTa-German-sentiment\": \"ssary/XLM-RoBERTa-German-sentiment\",\n",
    "               \"GermanFinBert_SC_Sentiment\": \"scherrmann/GermanFinBert_SC_Sentiment\",\n",
    "               \"twitter-xlm-roberta-base-sentiment-finetunned\": \"citizenlab/twitter-xlm-roberta-base-sentiment-finetunned\"}\n",
    "\n",
    "# Set up tokenizers with truncation for each model\n",
    "max_token_length = 512\n",
    "tokenizers = {name: AutoTokenizer.from_pretrained(model) for name, model in model_names.items()}\n",
    "\n",
    "# Truncate each text if it exceeds the maximum length for each model\n",
    "def truncate_text(text, tokenizer):\n",
    "    tokens = tokenizer.encode(text, truncation=True, max_length=max_token_length)\n",
    "    return tokenizer.decode(tokens, skip_special_tokens=True)\n",
    "\n",
    "# Apply truncation for each model's processed content\n",
    "for model_name, tokenizer in tokenizers.items():\n",
    "    sample_df[f'processed_content_{model_name}'] = sample_df['processed_content'].apply(lambda x: truncate_text(x, tokenizer))\n",
    "\n",
    "# Step 4: Load pipelines for each model with max_length and truncation set\n",
    "pipelines = {\n",
    "    name: pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer, max_length=max_token_length, truncation=True)\n",
    "    for name, (model, tokenizer) in zip(model_names.keys(), zip(model_names.values(), tokenizers.values()))}\n",
    "\n",
    "# Apply each model's pipeline to the truncated text and save results in separate columns\n",
    "for model_name, sentiment_pipeline in pipelines.items():\n",
    "    sample_df[f'sentiment_{model_name}'] = sample_df[f'processed_content_{model_name}'].apply(\n",
    "        lambda x: sentiment_pipeline(x)[0]['label'])\n",
    "\n",
    "# Step 5: Save the sample DataFrame with sentiment results to a new CSV file\n",
    "sample_df.to_csv(\"sample_processed_articles_with_multiple_sentiments.csv\", index=False)\n",
    "\n",
    "print(\"Sentiment analysis with multiple models, including the Twitter model, completed and results saved to 'sample_processed_articles_with_multiple_sentiments.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c8ffa7e-c99f-40eb-9ded-b30fb4990d53",
   "metadata": {},
   "source": [
    "# Sentiment analysis by sources and topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "305c45a1-6a89-4618-9276-608d67b207d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/reppmazc/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment analysis completed for top topics and sources. Results saved to 'filtered_topic_sentiment_test.csv'\n",
      "Sentiment distribution by top topics and sources:\n",
      "sentiment           negative   neutral  positive\n",
      "topic source                                    \n",
      "0     bild          0.000000  1.000000  0.000000\n",
      "      faz           0.000000  1.000000  0.000000\n",
      "      morgenpost    0.000000  1.000000  0.000000\n",
      "      stern         0.111111  0.777778  0.111111\n",
      "      sueddeutsche  0.000000  1.000000  0.000000\n",
      "...                      ...       ...       ...\n",
      "15    tagesschau    0.000000  1.000000  0.000000\n",
      "      welt          1.000000  0.000000  0.000000\n",
      "      zeit          0.000000  1.000000  0.000000\n",
      "16    faz           0.000000  1.000000  0.000000\n",
      "      jungewelt     1.000000  0.000000  0.000000\n",
      "\n",
      "[159 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# Load the datasets\n",
    "articles_df = pd.read_csv('/Users/reppmazc/Documents/IRONHACK/quests/final_project/df_toKenized_topic.csv')\n",
    "topics_df = pd.read_csv('/Users/reppmazc/Documents/IRONHACK/quests/final_project/df_topics_edited.csv')\n",
    "\n",
    "# Convert column names to lowercase for consistency\n",
    "topics_df.columns = map(str.lower, topics_df.columns)\n",
    "articles_df.columns = map(str.lower, articles_df.columns)\n",
    "\n",
    "# Merge the dataframes on the 'topic' column using a left join\n",
    "merged_df = articles_df.merge(topics_df, on='topic', how='left')\n",
    "\n",
    "# Ensure stopwords are downloaded\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('german'))\n",
    "\n",
    "# Step 1: Identify the top 3 most common topics (excluding -1) and the top 3 sources\n",
    "top_topics = merged_df[~merged_df['topic'].isin([-1, 2])]['topic'].value_counts().head(16).index\n",
    "top_sources = merged_df['source'].value_counts().head(18).index\n",
    "\n",
    "# Step 2: Filter the dataset for these topics and sources\n",
    "filtered_df = merged_df[(merged_df['topic'].isin(top_topics)) & \n",
    "                        (merged_df['source'].isin(top_sources))]\n",
    "\n",
    "# Optional: Limit rows further if necessary\n",
    "filtered_df = filtered_df.sample(n=1000, random_state=1)  # Adjust sample size if needed\n",
    "\n",
    "# Step 3: Define text preprocessing function\n",
    "def preprocess_text(text):\n",
    "    text = re.sub(r'\\d+', '', text)  # Remove numbers\n",
    "    text = re.sub(r'\\W', ' ', text)  # Remove punctuation\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    tokens = simple_preprocess(text)  # Tokenize\n",
    "    tokens = [word for word in tokens if word not in stop_words]  # Remove stopwords\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Apply preprocessing to the 'content' column\n",
    "filtered_df['processed_content'] = filtered_df['content'].apply(preprocess_text)\n",
    "\n",
    "# Step 4: Load tokenizer and pipeline for one model\n",
    "model_name = \"ssary/XLM-RoBERTa-German-sentiment\"  # Select one model for testing\n",
    "max_token_length = 512\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Truncate text for tokenization\n",
    "def truncate_text(text, tokenizer):\n",
    "    tokens = tokenizer.encode(text, truncation=True, max_length=max_token_length)\n",
    "    return tokenizer.decode(tokens, skip_special_tokens=True)\n",
    "\n",
    "filtered_df['processed_content'] = filtered_df['processed_content'].apply(lambda x: truncate_text(x, tokenizer))\n",
    "\n",
    "# Load sentiment analysis pipeline\n",
    "sentiment_pipeline = pipeline(\"sentiment-analysis\", model=model_name, tokenizer=tokenizer, max_length=max_token_length, truncation=True)\n",
    "\n",
    "# Step 5: Apply the model to get sentiment\n",
    "filtered_df['sentiment'] = filtered_df['processed_content'].apply(lambda x: sentiment_pipeline(x)[0]['label'])\n",
    "\n",
    "# Step 6: Save filtered and processed results to CSV\n",
    "#filtered_df.to_csv(f\"filtered_topic_sentiment_test_{model_name.split('/')[0]}.csv\", index=False)\n",
    "#print(\"Sentiment analysis completed for top topics and sources. Results saved to 'filtered_topic_sentiment_test.csv'\")\n",
    "\n",
    "# Step 7: Analyze sentiment by topic and source\n",
    "sentiment_by_topic_source = filtered_df.groupby(['topic', 'source'])['sentiment'].value_counts(normalize=True).unstack(fill_value=0)\n",
    "print(\"Sentiment distribution by top topics and sources:\")\n",
    "print(sentiment_by_topic_source)\n",
    "\n",
    "# Optional: Save distribution table to CSV\n",
    "sentiment_by_topic_source.to_csv(f\"filtered_sentiment_distribution_by_topic_source_{model_name.split('/')[0]}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "98922669-a4b5-4e15-8e6a-ba0030ae6540",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/reppmazc/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7907\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment analysis completed for top topics and sources. Results saved to 'filtered_topic_sentiment_test_ssary.csv'\n",
      "Sentiment distribution by top topics and sources:\n",
      "sentiment                                      negative   neutral  positive\n",
      "topic                            source                                    \n",
      "0_fc_trainer_fußball_bundesliga  bild          0.266667  0.666667  0.066667\n",
      "                                 dw            0.142857  0.857143  0.000000\n",
      "                                 faz           0.125000  0.812500  0.062500\n",
      "                                 focus         0.000000  1.000000  0.000000\n",
      "                                 jungewelt     0.000000  1.000000  0.000000\n",
      "...                                                 ...       ...       ...\n",
      "9_euro_millionen_milliarden_jahr sueddeutsche  0.250000  0.750000  0.000000\n",
      "                                 tagesschau    0.000000  1.000000  0.000000\n",
      "                                 tagesspiegel  0.064516  0.935484  0.000000\n",
      "                                 welt          0.254386  0.745614  0.000000\n",
      "                                 zeit          0.302083  0.697917  0.000000\n",
      "\n",
      "[228 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.utils import simple_preprocess\n",
    "from transformers import AutoTokenizer, pipeline\n",
    "\n",
    "# Load the datasets\n",
    "articles_df = pd.read_csv('/Users/reppmazc/Documents/IRONHACK/quests/final_project/df_toKenized_topic.csv')\n",
    "topics_df = pd.read_csv('/Users/reppmazc/Documents/IRONHACK/quests/final_project/df_topics.csv')\n",
    "\n",
    "# Convert column names to lowercase for consistency\n",
    "topics_df.columns = map(str.lower, topics_df.columns)\n",
    "articles_df.columns = map(str.lower, articles_df.columns)\n",
    "\n",
    "# Merge the dataframes on the 'topic' column using a left join\n",
    "merged_df = articles_df.merge(topics_df, on='topic', how='left')\n",
    "\n",
    "merged_df.to_csv('/Users/reppmazc/Documents/IRONHACK/quests/final_project/topics.csv')\n",
    "\n",
    "# Ensure stopwords are downloaded\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('german'))\n",
    "\n",
    "# Step 1: Identify top topics with 'count' > 200 but <= 1000, and top sources appearing more than 90 times\n",
    "top_topics = merged_df[(merged_df['count'] > 200) & (merged_df['count'] <= 1000)]['topic'].unique()\n",
    "top_sources = merged_df['source'].value_counts()[lambda x: x > 90].index\n",
    "\n",
    "# Step 2: Filter the dataset for these topics and sources\n",
    "filtered_df = merged_df[(merged_df['topic'].isin(top_topics)) & \n",
    "                        (merged_df['source'].isin(top_sources))]\n",
    "\n",
    "filtered_size = len(filtered_df)\n",
    "print(filtered_size)\n",
    "sample_size = min(10000, filtered_size)  # Use 10,000 if possible, or the maximum available\n",
    "\n",
    "# Sample with the adjusted size\n",
    "filtered_df = filtered_df.sample(n=sample_size, random_state=1)\n",
    "\n",
    "# Step 3: Define text preprocessing function\n",
    "def preprocess_text(text):\n",
    "    text = re.sub(r'\\d+', '', text)  # Remove numbers\n",
    "    text = re.sub(r'\\W', ' ', text)  # Remove punctuation\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    tokens = simple_preprocess(text)  # Tokenize\n",
    "    tokens = [word for word in tokens if word not in stop_words]  # Remove stopwords\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Apply preprocessing to the 'content' column\n",
    "filtered_df['processed_content'] = filtered_df['content'].apply(preprocess_text)\n",
    "\n",
    "# Step 4: Load tokenizer and pipeline for one model\n",
    "model_name = \"ssary/XLM-RoBERTa-German-sentiment\"  # Select one model for testing\n",
    "max_token_length = 512\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Truncate text for tokenization\n",
    "def truncate_text(text, tokenizer):\n",
    "    tokens = tokenizer.encode(text, truncation=True, max_length=max_token_length)\n",
    "    return tokenizer.decode(tokens, skip_special_tokens=True)\n",
    "\n",
    "filtered_df['processed_content'] = filtered_df['processed_content'].apply(lambda x: truncate_text(x, tokenizer))\n",
    "\n",
    "# Load sentiment analysis pipeline\n",
    "sentiment_pipeline = pipeline(\"sentiment-analysis\", model=model_name, tokenizer=tokenizer, max_length=max_token_length, truncation=True)\n",
    "\n",
    "# Step 5: Apply the model to get sentiment\n",
    "filtered_df['sentiment'] = filtered_df['processed_content'].apply(lambda x: sentiment_pipeline(x)[0]['label'])\n",
    "\n",
    "# Replace topic numbers with topic names in the final results\n",
    "filtered_df['topic'] = filtered_df['name']  # Assuming 'name' column in topics_df contains the topic name\n",
    "\n",
    "# Step 6: Save filtered and processed results to CSV\n",
    "filtered_df.to_csv(f\"filtered_topic_sentiment_test_{model_name.split('/')[0]}.csv\", index=False)\n",
    "print(f\"Sentiment analysis completed for top topics and sources. Results saved to 'filtered_topic_sentiment_test_{model_name.split('/')[0]}.csv'\")\n",
    "\n",
    "# Step 7: Analyze sentiment by topic and source\n",
    "sentiment_by_topic_source = filtered_df.groupby(['topic', 'source'])['sentiment'].value_counts(normalize=True).unstack(fill_value=0)\n",
    "print(\"Sentiment distribution by top topics and sources:\")\n",
    "print(sentiment_by_topic_source)\n",
    "\n",
    "# Optional: Save distribution table to CSV with topic names\n",
    "sentiment_by_topic_source.to_csv(f\"filtered_sentiment_distribution_by_topic_source_{model_name.split('/')[0]}.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86370e59-569a-48c6-920d-8179ddce78f7",
   "metadata": {},
   "source": [
    "# political leaning classification with BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f674200c-4be7-4654-929b-6a8b4161e48e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83b8be9b-2981-478d-bc7a-f413de898fdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/4j/klr6l0gn60b9blsxb73r3jq40000gn/T/ipykernel_6536/1911627846.py:15: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df = df.groupby('label').apply(lambda x: x.sample(n=800, random_state=42)).reset_index(drop=True)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"/Users/reppmazc/Documents/IRONHACK/quests/final_project/cleaned_articles_wo_date.csv\")\n",
    "\n",
    "# Map sources to political spectrum labels\n",
    "source_to_label = {\"taz\": \"links\", \"jungewelt\": \"links\", \"freitag\": \"links\", \"sueddeutsche\": \"links\",\n",
    "                   \"dw\": \"mitte\", \"tagesschau\": \"mitte\",\n",
    "                   \"bild\": \"rechts\", \"focus\": \"rechts\", \"welt\": \"rechts\", \"jungefreiheit\": \"rechts\"}\n",
    "\n",
    "# Apply the mapping to the 'label' column\n",
    "df['label'] = df['source'].map(source_to_label)\n",
    "\n",
    "# Drop any rows where label is NaN (in case of unmatched sources)\n",
    "df = df.dropna(subset=['label'])\n",
    "\n",
    "# Sample 750 articles per spectrum label to create a balanced dataset\n",
    "df = df.groupby('label').apply(lambda x: x.sample(n=800, random_state=42)).reset_index(drop=True)\n",
    "\n",
    "# Fill any NaN in content with an empty string (to avoid errors)\n",
    "df['content'] = df['content'].fillna(\"\")\n",
    "\n",
    "# Encode Labels\n",
    "label_encoder = LabelEncoder()\n",
    "df['label'] = label_encoder.fit_transform(df['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c8e7cf39-3911-46f6-83e0-5567c0174b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train and test sets\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert DataFrames to Hugging Face Dataset format\n",
    "train_dataset = Dataset.from_pandas(train_df[['content', 'label']])\n",
    "test_dataset = Dataset.from_pandas(test_df[['content', 'label']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a6b862f-64e5-4f77-b7bc-cb49dfa97825",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-german-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afc827a036614ad5b31694163cfe1953",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1920 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "340b2ed35a3b467da777b4ca2be94252",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/480 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load BERT tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('dbmdz/bert-base-german-cased')\n",
    "model = BertForSequenceClassification.from_pretrained('dbmdz/bert-base-german-cased', num_labels=len(label_encoder.classes_))\n",
    "\n",
    "# Tokenize the dataset\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['content'], padding=\"max_length\", truncation=True, max_length=256)\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "test_dataset = test_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Set the format for PyTorch\n",
    "train_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "test_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "46b859fb-7446-4e9e-b366-fbf7394943ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/transformers_test/lib/python3.9/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Define Training Arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10)\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(model=model,\n",
    "                  args=training_args,\n",
    "                  train_dataset=train_dataset,\n",
    "                  eval_dataset=test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bb935f41-b846-4268-a539-2692846288b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='720' max='720' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [720/720 28:55, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.280600</td>\n",
       "      <td>0.294454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.302200</td>\n",
       "      <td>0.325230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.001500</td>\n",
       "      <td>0.302689</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='60' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [60/60 00:36]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results: {'eval_loss': 0.3026885390281677, 'eval_runtime': 36.7475, 'eval_samples_per_second': 13.062, 'eval_steps_per_second': 1.633, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate the model\n",
    "results = trainer.evaluate()\n",
    "\n",
    "# Print the evaluation results\n",
    "print(\"Evaluation Results:\", results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43bf2e65-9211-4c1a-be60-fd35c5a7f24b",
   "metadata": {},
   "source": [
    "# political leaning classification with BERT (2 epochs only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e325ca1-0e27-4742-951c-56a245efb362",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/4j/klr6l0gn60b9blsxb73r3jq40000gn/T/ipykernel_1659/4259393649.py:24: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df = df.groupby('label').apply(lambda x: x.sample(n=800, random_state=42)).reset_index(drop=True)\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-german-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14bcd567a0cf40bda5042f74c09e4450",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1920 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbb22d4cd2af45b6a7e18c50d96082b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/480 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/transformers_test/lib/python3.9/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='480' max='480' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [480/480 17:44, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.181400</td>\n",
       "      <td>0.216879</td>\n",
       "      <td>0.920833</td>\n",
       "      <td>0.920894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.224300</td>\n",
       "      <td>0.248046</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.933016</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results: {'eval_loss': 0.2480456829071045, 'eval_accuracy': 0.9333333333333333, 'eval_f1': 0.9330157533797034, 'eval_runtime': 33.7792, 'eval_samples_per_second': 14.21, 'eval_steps_per_second': 1.776, 'epoch': 2.0}\n",
      "Model, tokenizer, and label encoder saved to ./saved_model\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from transformers import Trainer, TrainingArguments, BertTokenizer, BertForSequenceClassification\n",
    "from datasets import Dataset\n",
    "\n",
    "df = pd.read_csv(\"/Users/reppmazc/Documents/IRONHACK/quests/final_project/cleaned_articles_wo_date.csv\")\n",
    "\n",
    "# Map sources to political spectrum labels\n",
    "source_to_label = {\"taz\": \"links\", \"jungewelt\": \"links\", \"freitag\": \"links\", \"sueddeutsche\": \"links\",\n",
    "                   \"dw\": \"mitte\", \"tagesschau\": \"mitte\",\n",
    "                   \"bild\": \"rechts\", \"focus\": \"rechts\", \"welt\": \"rechts\", \"jungefreiheit\": \"rechts\"}\n",
    "\n",
    "# Apply the mapping to the 'label' column\n",
    "df['label'] = df['source'].map(source_to_label)\n",
    "\n",
    "# Drop any rows where label is NaN (in case of unmatched sources)\n",
    "df = df.dropna(subset=['label'])\n",
    "\n",
    "# Sample 750 articles per spectrum label to create a balanced dataset\n",
    "df = df.groupby('label').apply(lambda x: x.sample(n=800, random_state=42)).reset_index(drop=True)\n",
    "\n",
    "# Fill any NaN in content with an empty string (to avoid errors)\n",
    "df['content'] = df['content'].fillna(\"\")\n",
    "\n",
    "# Encode Labels\n",
    "label_encoder = LabelEncoder()\n",
    "df['label'] = label_encoder.fit_transform(df['label'])\n",
    "\n",
    "# Split into train and test sets\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert DataFrames to Hugging Face Dataset format\n",
    "train_dataset = Dataset.from_pandas(train_df[['content', 'label']])\n",
    "test_dataset = Dataset.from_pandas(test_df[['content', 'label']])\n",
    "\n",
    "# Load BERT tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('dbmdz/bert-base-german-cased')\n",
    "model = BertForSequenceClassification.from_pretrained('dbmdz/bert-base-german-cased', num_labels=len(label_encoder.classes_))\n",
    "\n",
    "# Tokenize the dataset\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['content'], padding=\"max_length\", truncation=True, max_length=256)\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "test_dataset = test_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Set the format for PyTorch\n",
    "train_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "test_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "\n",
    "# Define the compute_metrics function for accuracy and F1-score\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = np.argmax(pred.predictions, axis=1)\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    f1 = f1_score(labels, preds, average='weighted')  # 'weighted' for multi-class\n",
    "    return {\"accuracy\": acc, \"f1\": f1}\n",
    "\n",
    "# Define Training Arguments with only 2 epochs\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=2,  # Set to 2 epochs\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10)\n",
    "\n",
    "# Initialize Trainer with the compute_metrics function\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=compute_metrics)\n",
    "\n",
    "# Train the model (will stop at 2 epochs)\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "results = trainer.evaluate()\n",
    "\n",
    "# Print the evaluation results, including accuracy and F1-score\n",
    "print(\"Evaluation Results:\", results)\n",
    "\n",
    "# Save the trained model and tokenizer\n",
    "model_save_path = \"./saved_model\"\n",
    "model.save_pretrained(model_save_path)\n",
    "tokenizer.save_pretrained(model_save_path)\n",
    "\n",
    "# Save the label encoder (for decoding predictions later)\n",
    "import joblib\n",
    "joblib.dump(label_encoder, f\"{model_save_path}/label_encoder.joblib\")\n",
    "\n",
    "print(f\"Model, tokenizer, and label encoder saved to {model_save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "584925f2-9410-4751-8b55-a906f19de65d",
   "metadata": {},
   "source": [
    "check predicted vs true label"
   ]
  },
  {
   "cell_type": "raw",
   "id": "691f7f83-1a14-49ec-9d58-11c09fc0a6ce",
   "metadata": {},
   "source": [
    "# Get predictions on the test dataset\n",
    "test_predictions = trainer.predict(test_dataset)\n",
    "\n",
    "# Extract predicted labels and true labels\n",
    "predicted_indices = np.argmax(test_predictions.predictions, axis=1)\n",
    "predicted_labels = label_encoder.inverse_transform(predicted_indices)\n",
    "true_indices = test_predictions.label_ids\n",
    "true_labels = label_encoder.inverse_transform(true_indices)\n",
    "\n",
    "# Print predicted vs. true labels\n",
    "for i in range(len(true_labels)):\n",
    "    print(f\"Article {i+1}:\")\n",
    "    print(f\"True label: {true_labels[i]}\")\n",
    "    print(f\"Predicted label: {predicted_labels[i]}\")\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db6013ac-67d8-45de-b8b6-467f9425b83e",
   "metadata": {},
   "source": [
    "apply model to new data without the sources used for initial political leaning labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "16161ba6-7199-4b32-9b60-8feac5826c6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered Data Shape: (18784, 5)\n",
      "Filtered Data Preview:     Unnamed: 0                                                url  \\\n",
      "2            2  https://www.faz.net/aktuell/politik/krieg-in-n...   \n",
      "3            3  https://www.tagesspiegel.de/berlin/bezirke/sch...   \n",
      "6            8  https://www.stern.de/gesellschaft/regional/sac...   \n",
      "7            9  https://www.faz.net/aktuell/politik/ausland/us...   \n",
      "10          12  https://www.stern.de/news/habeck-erwartet-haus...   \n",
      "\n",
      "                                              content  datetime        source  \n",
      "2   Krieg in Nahost : Israels heikle Optionen Ein ...       NaN           faz  \n",
      "3    Carsten Berger Das Angebot kam plötzlich: 20 ...       NaN  tagesspiegel  \n",
      "6    Link Der deutsche Meister SC Magdeburg bleibt...       NaN         stern  \n",
      "7   Wahlkampf in den USA : Nichts läuft nach Plan ...       NaN           faz  \n",
      "10   Link Vizekanzler Robert Habeck (Grüne) erwart...       NaN         stern  \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4351c944ca1c4ee5a2a44beceddcb1e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/18784 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         source predicted_label  percentage\n",
      "0    abendblatt           mitte  100.000000\n",
      "1      brigitte           links  100.000000\n",
      "2          chip          rechts  100.000000\n",
      "3   derstandard           links   50.000000\n",
      "4   derstandard          rechts   50.000000\n",
      "..          ...             ...         ...\n",
      "64  volksfreund          rechts   63.636364\n",
      "65  volksfreund           links   36.363636\n",
      "66         zeit           links   47.679255\n",
      "67         zeit          rechts   37.381467\n",
      "68         zeit           mitte   14.939278\n",
      "\n",
      "[69 rows x 3 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/4j/klr6l0gn60b9blsxb73r3jq40000gn/T/ipykernel_1659/1444059217.py:31: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  new_data_df['predicted_label'] = predicted_labels\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import Trainer, BertTokenizer, BertForSequenceClassification\n",
    "import joblib\n",
    "\n",
    "# Load the new data\n",
    "file_path = \"/Users/reppmazc/Documents/IRONHACK/quests/final_project/cleaned_articles_wo_date.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Define sources to exclude\n",
    "excluded_sources = [\"taz\", \"jungewelt\", \"freitag\", \"sueddeutsche\", \"dw\", \"tagesschau\", \"bild\", \"focus\", \"welt\", \"jungefreiheit\"]\n",
    "\n",
    "# Filter out rows with excluded sources\n",
    "new_data_df = df[~df['source'].isin(excluded_sources)]\n",
    "\n",
    "print(\"Filtered Data Shape:\", new_data_df.shape)\n",
    "print(\"Filtered Data Preview:\", new_data_df.head())\n",
    "\n",
    "# Load the saved model, tokenizer, and label encoder\n",
    "model_save_path = \"./saved_model\"\n",
    "model = BertForSequenceClassification.from_pretrained(model_save_path)\n",
    "tokenizer = BertTokenizer.from_pretrained(model_save_path)\n",
    "label_encoder = joblib.load(f\"{model_save_path}/label_encoder.joblib\")\n",
    "\n",
    "# Tokenize new data\n",
    "new_dataset = Dataset.from_pandas(new_data_df[['content']])\n",
    "\n",
    "def tokenize_new_data(examples):\n",
    "    return tokenizer(examples['content'], padding=\"max_length\", truncation=True, max_length=256)\n",
    "\n",
    "new_dataset = new_dataset.map(tokenize_new_data, batched=True)\n",
    "new_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask'])\n",
    "\n",
    "# Initialize Trainer with the loaded model\n",
    "trainer = Trainer(model=model)\n",
    "\n",
    "# Use the trainer to predict\n",
    "predictions = trainer.predict(new_dataset)\n",
    "\n",
    "# Extract predicted labels\n",
    "predicted_indices = np.argmax(predictions.predictions, axis=1)\n",
    "predicted_labels = label_encoder.inverse_transform(predicted_indices)\n",
    "\n",
    "# Add predictions to the DataFrame\n",
    "new_data_df['predicted_label'] = predicted_labels\n",
    "\n",
    "# Calculate label percentages per source\n",
    "label_distribution = (\n",
    "    new_data_df.groupby('source')['predicted_label']\n",
    "    .value_counts(normalize=True)\n",
    "    .rename(\"percentage\")\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Convert percentages to readable format\n",
    "label_distribution['percentage'] = label_distribution['percentage'] * 100\n",
    "\n",
    "# Display the distribution results\n",
    "print(label_distribution)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5cda9bf1-b644-4bca-b22c-1eb0c69cee53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the majority label for each source\n",
    "majority_labels = (\n",
    "    new_data_df.groupby('source')['predicted_label']\n",
    "    .agg(lambda x: x.value_counts().idxmax())\n",
    "    .reset_index()\n",
    "    .rename(columns={'predicted_label': 'majority_label'})\n",
    ")\n",
    "\n",
    "# Merge the majority label with the original label distribution\n",
    "label_distribution = label_distribution.merge(majority_labels, on='source')\n",
    "\n",
    "# Save the final result with majority label\n",
    "label_distribution.to_csv('/Users/reppmazc/Documents/IRONHACK/quests/final_project/pol_leaning_classification_dist_with_majority.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d383c4a4-b300-49d2-991c-60142646c0ed",
   "metadata": {},
   "source": [
    "# political leaning classification with BERT (2 epochs only) - changed initial lables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f894dbc1-4924-41a9-8299-7178164d9dfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/4j/klr6l0gn60b9blsxb73r3jq40000gn/T/ipykernel_1659/3971258823.py:24: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df = df.groupby('label').apply(lambda x: x.sample(n=800, random_state=42)).reset_index(drop=True)\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-german-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "324dc3523db44e2e80e9c9c04db7dd58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1920 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34b642262a464702b5be6ae7ba551653",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/480 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/transformers_test/lib/python3.9/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='480' max='480' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [480/480 19:33, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.329200</td>\n",
       "      <td>0.276519</td>\n",
       "      <td>0.895833</td>\n",
       "      <td>0.895979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.078200</td>\n",
       "      <td>0.287193</td>\n",
       "      <td>0.920833</td>\n",
       "      <td>0.920866</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='60' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [60/60 00:39]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results: {'eval_loss': 0.28719285130500793, 'eval_accuracy': 0.9208333333333333, 'eval_f1': 0.9208658439944357, 'eval_runtime': 40.414, 'eval_samples_per_second': 11.877, 'eval_steps_per_second': 1.485, 'epoch': 2.0}\n",
      "Model, tokenizer, and label encoder saved to ./saved_model_changed_lables\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from transformers import Trainer, TrainingArguments, BertTokenizer, BertForSequenceClassification\n",
    "from datasets import Dataset\n",
    "\n",
    "df = pd.read_csv(\"/Users/reppmazc/Documents/IRONHACK/quests/final_project/cleaned_articles_wo_date.csv\")\n",
    "\n",
    "# Map sources to political spectrum labels\n",
    "source_to_label = {\"spiegel\": \"links\", \"jungewelt\": \"links\", \"freitag\": \"links\", \"sueddeutsche\": \"links\",\n",
    "                   \"dw\": \"mitte\", \"tagesschau\": \"mitte\",\n",
    "                   \"bild\": \"rechts\", \"focus\": \"rechts\", \"welt\": \"rechts\", \"jungefreiheit\": \"rechts\"}\n",
    "\n",
    "# Apply the mapping to the 'label' column\n",
    "df['label'] = df['source'].map(source_to_label)\n",
    "\n",
    "# Drop any rows where label is NaN (in case of unmatched sources)\n",
    "df = df.dropna(subset=['label'])\n",
    "\n",
    "# Sample 750 articles per spectrum label to create a balanced dataset\n",
    "df = df.groupby('label').apply(lambda x: x.sample(n=800, random_state=42)).reset_index(drop=True)\n",
    "\n",
    "# Fill any NaN in content with an empty string (to avoid errors)\n",
    "df['content'] = df['content'].fillna(\"\")\n",
    "\n",
    "# Encode Labels\n",
    "label_encoder = LabelEncoder()\n",
    "df['label'] = label_encoder.fit_transform(df['label'])\n",
    "\n",
    "# Split into train and test sets\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert DataFrames to Hugging Face Dataset format\n",
    "train_dataset = Dataset.from_pandas(train_df[['content', 'label']])\n",
    "test_dataset = Dataset.from_pandas(test_df[['content', 'label']])\n",
    "\n",
    "# Load BERT tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('dbmdz/bert-base-german-cased')\n",
    "model = BertForSequenceClassification.from_pretrained('dbmdz/bert-base-german-cased', num_labels=len(label_encoder.classes_))\n",
    "\n",
    "# Tokenize the dataset\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['content'], padding=\"max_length\", truncation=True, max_length=256)\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "test_dataset = test_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Set the format for PyTorch\n",
    "train_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "test_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "\n",
    "# Define the compute_metrics function for accuracy and F1-score\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = np.argmax(pred.predictions, axis=1)\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    f1 = f1_score(labels, preds, average='weighted')  # 'weighted' for multi-class\n",
    "    return {\"accuracy\": acc, \"f1\": f1}\n",
    "\n",
    "# Define Training Arguments with only 2 epochs\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=2,  # Set to 2 epochs\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10)\n",
    "\n",
    "# Initialize Trainer with the compute_metrics function\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=compute_metrics)\n",
    "\n",
    "# Train the model (will stop at 2 epochs)\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "results = trainer.evaluate()\n",
    "\n",
    "# Print the evaluation results, including accuracy and F1-score\n",
    "print(\"Evaluation Results:\", results)\n",
    "\n",
    "# Save the trained model and tokenizer\n",
    "model_save_path = \"./saved_model_changed_lables\"\n",
    "model.save_pretrained(model_save_path)\n",
    "tokenizer.save_pretrained(model_save_path)\n",
    "\n",
    "# Save the label encoder (for decoding predictions later)\n",
    "import joblib\n",
    "joblib.dump(label_encoder, f\"{model_save_path}/label_encoder.joblib\")\n",
    "\n",
    "print(f\"Model, tokenizer, and label encoder saved to {model_save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9b869b19-2a1a-438d-bd4c-57c6ba5b0074",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered Data Shape: (18681, 5)\n",
      "Filtered Data Preview:     Unnamed: 0                                                url  \\\n",
      "2            2  https://www.faz.net/aktuell/politik/krieg-in-n...   \n",
      "3            3  https://www.tagesspiegel.de/berlin/bezirke/sch...   \n",
      "6            8  https://www.stern.de/gesellschaft/regional/sac...   \n",
      "7            9  https://www.faz.net/aktuell/politik/ausland/us...   \n",
      "10          12  https://www.stern.de/news/habeck-erwartet-haus...   \n",
      "\n",
      "                                              content  datetime        source  \n",
      "2   Krieg in Nahost : Israels heikle Optionen Ein ...       NaN           faz  \n",
      "3    Carsten Berger Das Angebot kam plötzlich: 20 ...       NaN  tagesspiegel  \n",
      "6    Link Der deutsche Meister SC Magdeburg bleibt...       NaN         stern  \n",
      "7   Wahlkampf in den USA : Nichts läuft nach Plan ...       NaN           faz  \n",
      "10   Link Vizekanzler Robert Habeck (Grüne) erwart...       NaN         stern  \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c158e257b1f7442eadd2b82bd4b84c49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/18681 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         source predicted_label  percentage\n",
      "0    abendblatt           mitte  100.000000\n",
      "1      brigitte           links  100.000000\n",
      "2          chip           links  100.000000\n",
      "3   derstandard           links   50.000000\n",
      "4   derstandard          rechts   50.000000\n",
      "..          ...             ...         ...\n",
      "60  volksfreund          rechts   45.454545\n",
      "61  volksfreund           mitte    1.010101\n",
      "62         zeit           links   52.720013\n",
      "63         zeit          rechts   37.215106\n",
      "64         zeit           mitte   10.064881\n",
      "\n",
      "[65 rows x 3 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/4j/klr6l0gn60b9blsxb73r3jq40000gn/T/ipykernel_1659/682641343.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  new_data_df['predicted_label'] = predicted_labels\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import Trainer, BertTokenizer, BertForSequenceClassification\n",
    "import joblib\n",
    "\n",
    "# Load the new data\n",
    "file_path = \"/Users/reppmazc/Documents/IRONHACK/quests/final_project/cleaned_articles_wo_date.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Define sources to exclude\n",
    "excluded_sources = [\"spiegel\", \"jungewelt\", \"freitag\", \"sueddeutsche\", \"dw\", \"tagesschau\", \"bild\", \"focus\", \"welt\", \"jungefreiheit\"]\n",
    "\n",
    "# Filter out rows with excluded sources\n",
    "new_data_df = df[~df['source'].isin(excluded_sources)]\n",
    "\n",
    "print(\"Filtered Data Shape:\", new_data_df.shape)\n",
    "print(\"Filtered Data Preview:\", new_data_df.head())\n",
    "\n",
    "# Load the saved model, tokenizer, and label encoder\n",
    "model_save_path = \"./saved_model_changed_lables\"\n",
    "model = BertForSequenceClassification.from_pretrained(model_save_path)\n",
    "tokenizer = BertTokenizer.from_pretrained(model_save_path)\n",
    "label_encoder = joblib.load(f\"{model_save_path}/label_encoder.joblib\")\n",
    "\n",
    "# Tokenize new data\n",
    "new_dataset = Dataset.from_pandas(new_data_df[['content']])\n",
    "\n",
    "def tokenize_new_data(examples):\n",
    "    return tokenizer(examples['content'], padding=\"max_length\", truncation=True, max_length=256)\n",
    "\n",
    "new_dataset = new_dataset.map(tokenize_new_data, batched=True)\n",
    "new_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask'])\n",
    "\n",
    "# Initialize Trainer with the loaded model\n",
    "trainer = Trainer(model=model)\n",
    "\n",
    "# Use the trainer to predict\n",
    "predictions = trainer.predict(new_dataset)\n",
    "\n",
    "# Extract predicted labels\n",
    "predicted_indices = np.argmax(predictions.predictions, axis=1)\n",
    "predicted_labels = label_encoder.inverse_transform(predicted_indices)\n",
    "\n",
    "# Add predictions to the DataFrame\n",
    "new_data_df['predicted_label'] = predicted_labels\n",
    "\n",
    "# Calculate label percentages per source\n",
    "label_distribution = (\n",
    "    new_data_df.groupby('source')['predicted_label']\n",
    "    .value_counts(normalize=True)\n",
    "    .rename(\"percentage\")\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Convert percentages to readable format\n",
    "label_distribution['percentage'] = label_distribution['percentage'] * 100\n",
    "\n",
    "# Display the distribution results\n",
    "print(label_distribution)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "59010f86-0b3b-4182-a7e7-5a6a66413c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the majority label for each source\n",
    "majority_labels = (\n",
    "    new_data_df.groupby('source')['predicted_label']\n",
    "    .agg(lambda x: x.value_counts().idxmax())\n",
    "    .reset_index()\n",
    "    .rename(columns={'predicted_label': 'majority_label'})\n",
    ")\n",
    "\n",
    "# Merge the majority label with the original label distribution\n",
    "label_distribution = label_distribution.merge(majority_labels, on='source')\n",
    "\n",
    "# Save the final result with majority label\n",
    "label_distribution.to_csv('/Users/reppmazc/Documents/IRONHACK/quests/final_project/pol_leaning_classification_dist_with_majority_new.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a91d8726-4760-4637-8713-b7dae4d9f4f3",
   "metadata": {},
   "source": [
    "# coding leaning manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1337697d-802c-4c15-ba39-7b35c61af02c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate 'pol_leaning' column based on 'source' column, accoroding to 'source_to_label' dictionary\n",
    "import pandas as pd\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_csv('/Users/reppmazc/Documents/IRONHACK/quests/final_project/cleaned_articles_wo_date.csv')\n",
    "\n",
    "# Define the source to political leaning mapping\n",
    "source_to_label = {\n",
    "    \"spiegel\": \"mitte_links\",\n",
    "    \"taz\": \"links\",\n",
    "    \"jungewelt\": \"links\",\n",
    "    \"freitag\": \"links\",\n",
    "    \"zeit\": \"mitte_links\",\n",
    "    \"tagesspiegel\": \"mitte_links\",\n",
    "    \"dw\": \"mitte\",\n",
    "    \"tagesschau\": \"mitte\",\n",
    "    \"stern\": \"mitte_rechts\",\n",
    "    \"focus\": \"mitte_rechts\",\n",
    "    \"welt\": \"mitte_rechts\",\n",
    "    \"jungefreiheit\": \"rechts\",\n",
    "    \"sueddeutsche\": \"mitte_links\",\n",
    "    \"faz\": \"mitte_rechts\",\n",
    "    \"morgenpost\": \"mitte_rechts\",\n",
    "    \"bild\": \"rechts\",\n",
    "    \"rbb24\": \"mitte\",\n",
    "    \"express\": \"rechts\"\n",
    "}\n",
    "\n",
    "# Create 'pol_leaning' column based on 'source' column using the dictionary\n",
    "df['pol_leaning'] = df['source'].map(source_to_label)\n",
    "\n",
    "# Save the updated DataFrame to a new CSV\n",
    "df.to_csv('/Users/reppmazc/Documents/IRONHACK/quests/final_project/cleaned_articles_wo_date_leaning.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66bad833-0486-446d-a73a-ee8f054309f7",
   "metadata": {},
   "source": [
    "# semtiment analysis on all data (only roberta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "46123236-783b-47ca-9e02-117c740f031e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/reppmazc/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment analysis with XLM-RoBERTa model completed and results saved to 'all_processed_articles_with_XLM_RoBERTa_sentiment.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from transformers import AutoTokenizer, pipeline\n",
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "# Ensure stopwords are downloaded\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('german'))\n",
    "\n",
    "# Step 1: Define text preprocessing function\n",
    "def preprocess_text(text):\n",
    "    # Remove numbers and punctuation, lowercase, and remove stopwords\n",
    "    text = re.sub(r'\\d+', '', text)  # Remove numbers\n",
    "    text = re.sub(r'\\W', ' ', text)  # Remove punctuation\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    tokens = simple_preprocess(text)  # Tokenize\n",
    "    tokens = [word for word in tokens if word not in stop_words]  # Remove stopwords\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# loading article data\n",
    "df = pd.read_csv(\"/Users/reppmazc/Documents/IRONHACK/quests/final_project/cleaned_articles_wo_date.csv\")\n",
    "\n",
    "# Apply preprocessing to the 'content' column (Consider using batch processing if memory is an issue)\n",
    "df['processed_content'] = df['content'].apply(preprocess_text)\n",
    "\n",
    "# Step 2: Load the tokenizer and sentiment analysis pipeline for the selected model\n",
    "model_name = \"ssary/XLM-RoBERTa-German-sentiment\"\n",
    "max_token_length = 512\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "sentiment_pipeline = pipeline(\"sentiment-analysis\", model=model_name, tokenizer=tokenizer, max_length=max_token_length, truncation=True)\n",
    "\n",
    "# Truncate text function\n",
    "def truncate_text(text):\n",
    "    tokens = tokenizer.encode(text, truncation=True, max_length=max_token_length)\n",
    "    return tokenizer.decode(tokens, skip_special_tokens=True)\n",
    "\n",
    "# Apply truncation to the processed content\n",
    "df['processed_content_truncated'] = df['processed_content'].apply(truncate_text)\n",
    "\n",
    "# Step 3: Apply the sentiment pipeline to each row in a memory-efficient way\n",
    "# Process data in chunks if the dataset is too large for your laptop's memory\n",
    "batch_size = 50  # You can adjust this based on your system's memory capacity\n",
    "sentiment_results = []\n",
    "\n",
    "for start in range(0, len(df), batch_size):\n",
    "    batch_texts = df['processed_content_truncated'][start:start+batch_size].tolist()\n",
    "    batch_sentiments = sentiment_pipeline(batch_texts)\n",
    "    sentiment_results.extend([result['label'] for result in batch_sentiments])\n",
    "\n",
    "# Save sentiment results to the DataFrame\n",
    "df['sentiment_XLM_RoBERTa'] = sentiment_results\n",
    "\n",
    "# Step 4: Save the entire DataFrame with sentiment results to a new CSV file\n",
    "df.to_csv(\"all_processed_articles_with_XLM_RoBERTa_sentiment.csv\", index=False)\n",
    "\n",
    "print(\"Sentiment analysis with XLM-RoBERTa model completed and results saved to 'all_processed_articles_with_XLM_RoBERTa_sentiment.csv'\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Transformers Test",
   "language": "python",
   "name": "transformers_test"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
