{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c71330fb-063d-4782-92dd-02bb91faf265",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.utils import simple_preprocess\n",
    "from transformers import pipeline, AutoTokenizer\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a918b8da-152e-4c34-ab0e-d6f14c9ff736",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading article data\n",
    "df = pd.read_csv(\"/Users/reppmazc/Documents/IRONHACK/quests/final_project/cleaned_articles_wo_date.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ec65302c-81c9-4917-b8ff-bc006455787b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/reppmazc/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ec36ddec20141d48d9be5e2712dfc55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/618 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e358fb8c938c4d86bd95275d0e3f69c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01a73d9cba9f42fc838013279b1b2a38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.10M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f4677dc65d1422a9762e4468a251b3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/150 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n",
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dba6167386414063bef6c81a34e7de65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/992 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d41b393311f416583d66ab9ec739c41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/1.11G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2205052d0c2f481fa848d2ead8a32498",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.11G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment analysis with multiple models, including the Twitter model, completed and results saved to 'sample_processed_articles_with_multiple_sentiments.csv'\n"
     ]
    }
   ],
   "source": [
    "# test three different models on 50 randomly selected articles\n",
    "# Ensure stopwords are downloaded\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('german'))\n",
    "\n",
    "# Step 1: Define text preprocessing function\n",
    "def preprocess_text(text):\n",
    "    # Remove numbers and punctuation, lowercase, and remove stopwords\n",
    "    text = re.sub(r'\\d+', '', text)  # Remove numbers\n",
    "    text = re.sub(r'\\W', ' ', text)  # Remove punctuation\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    tokens = simple_preprocess(text)  # Tokenize\n",
    "    tokens = [word for word in tokens if word not in stop_words]  # Remove stopwords\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Apply preprocessing to the 'content' column\n",
    "df['processed_content'] = df['content'].apply(preprocess_text)\n",
    "\n",
    "# Step 2: Select a random sample of 100 articles\n",
    "sample_df = df.sample(n=100, random_state=1)\n",
    "\n",
    "# Step 3: Load tokenizers and define truncation for each model\n",
    "model_names = {\"XLM-RoBERTa-German-sentiment\": \"ssary/XLM-RoBERTa-German-sentiment\",\n",
    "               \"GermanFinBert_SC_Sentiment\": \"scherrmann/GermanFinBert_SC_Sentiment\",\n",
    "               \"twitter-xlm-roberta-base-sentiment-finetunned\": \"citizenlab/twitter-xlm-roberta-base-sentiment-finetunned\"}\n",
    "\n",
    "# Set up tokenizers with truncation for each model\n",
    "max_token_length = 512\n",
    "tokenizers = {name: AutoTokenizer.from_pretrained(model) for name, model in model_names.items()}\n",
    "\n",
    "# Truncate each text if it exceeds the maximum length for each model\n",
    "def truncate_text(text, tokenizer):\n",
    "    tokens = tokenizer.encode(text, truncation=True, max_length=max_token_length)\n",
    "    return tokenizer.decode(tokens, skip_special_tokens=True)\n",
    "\n",
    "# Apply truncation for each model's processed content\n",
    "for model_name, tokenizer in tokenizers.items():\n",
    "    sample_df[f'processed_content_{model_name}'] = sample_df['processed_content'].apply(lambda x: truncate_text(x, tokenizer))\n",
    "\n",
    "# Step 4: Load pipelines for each model with max_length and truncation set\n",
    "pipelines = {\n",
    "    name: pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer, max_length=max_token_length, truncation=True)\n",
    "    for name, (model, tokenizer) in zip(model_names.keys(), zip(model_names.values(), tokenizers.values()))}\n",
    "\n",
    "# Apply each model's pipeline to the truncated text and save results in separate columns\n",
    "for model_name, sentiment_pipeline in pipelines.items():\n",
    "    sample_df[f'sentiment_{model_name}'] = sample_df[f'processed_content_{model_name}'].apply(\n",
    "        lambda x: sentiment_pipeline(x)[0]['label'])\n",
    "\n",
    "# Step 5: Save the sample DataFrame with sentiment results to a new CSV file\n",
    "sample_df.to_csv(\"sample_processed_articles_with_multiple_sentiments.csv\", index=False)\n",
    "\n",
    "print(\"Sentiment analysis with multiple models, including the Twitter model, completed and results saved to 'sample_processed_articles_with_multiple_sentiments.csv'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Transformers Test",
   "language": "python",
   "name": "transformers_test"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
